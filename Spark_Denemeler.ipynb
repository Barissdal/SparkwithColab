{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9/yIRIO2K3ie/7oEjRwKe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Barissdal/SparkwithColab/blob/main/Spark_Denemeler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCMYl0Y-ExMV",
        "outputId": "a8f70b2a-5ca2-49c3-b0fc-23de12f62232"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,058 kB]\n",
            "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,354 kB]\n",
            "Hit:13 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Fetched 2,748 kB in 2s (1,441 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "KPfbEFCeE0aE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q http://apache.osuosl.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "-AtTKeBKJBc0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.4.0-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "QTSeOxpXE9qM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCpjaUZZFaGy",
        "outputId": "d02c667f-b1b0-4e77-df7a-29e5ef5d832f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SPARK_VERSION = '3.4.0'"
      ],
      "metadata": {
        "id": "6bcnbmvhJlYJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.0-bin-hadoop3\"\n",
        "os.environ[\"SPARK_VERSION\"] = '3.4.0'"
      ],
      "metadata": {
        "id": "Mzo6GucIFdty"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5JxNs53FtNh",
        "outputId": "a487d2ac-cf99-44ac-c8ae-e1c5d690dceb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "DiIie4NKFzFQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "idUX42GMJx6p",
        "outputId": "2cfcc06f-c8a2-449b-cb0a-6cddaa77ad00"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.4.0-bin-hadoop3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('rev').getOrCreate()\n",
        "employee = [(\"Radhika\",10), \n",
        "        (\"Kaavya\",20), \n",
        "        (\"Varnika\",30), \n",
        "        (\"Akshay\",40) \n",
        "      ]\n",
        "rdd = spark.sparkContext.parallelize(employee)"
      ],
      "metadata": {
        "id": "_1N6DPeCJ2sp"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframee = rdd.toDF()\n",
        "dataframee.printSchema()\n",
        "dataframee.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6r-bGl4J8no",
        "outputId": "a591a2dc-2c99-479f-f91c-4a544f57a833"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _1: string (nullable = true)\n",
            " |-- _2: long (nullable = true)\n",
            "\n",
            "+-------+---+\n",
            "|     _1| _2|\n",
            "+-------+---+\n",
            "|Radhika| 10|\n",
            "| Kaavya| 20|\n",
            "|Varnika| 30|\n",
            "| Akshay| 40|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp_columns = [\"emp_name\",\"emp_id\"]\n",
        "df = rdd.toDF(emp_columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx8n28JIKGrA",
        "outputId": "bdebadd1-584f-45cf-caea-78c75bad8931"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- emp_name: string (nullable = true)\n",
            " |-- emp_id: long (nullable = true)\n",
            "\n",
            "+--------+------+\n",
            "|emp_name|emp_id|\n",
            "+--------+------+\n",
            "|Radhika |10    |\n",
            "|Kaavya  |20    |\n",
            "|Varnika |30    |\n",
            "|Akshay  |40    |\n",
            "+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType\n",
        "empSchema = StructType([       \n",
        "    StructField('emp_name', StringType(), True),\n",
        "    StructField('emp_id', StringType(), True)\n",
        "])\n",
        "\n",
        "employee_df = spark.createDataFrame(data=employee, schema = empSchema)\n",
        "employee_df.printSchema()\n",
        "employee_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5GF0DFKKiye",
        "outputId": "95095264-b3d6-4a6d-963a-bc3b87d047e7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- emp_name: string (nullable = true)\n",
            " |-- emp_id: string (nullable = true)\n",
            "\n",
            "+--------+------+\n",
            "|emp_name|emp_id|\n",
            "+--------+------+\n",
            "|Radhika |10    |\n",
            "|Kaavya  |20    |\n",
            "|Varnika |30    |\n",
            "|Akshay  |40    |\n",
            "+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "employee_df.groupBy('emp_name').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGrlGHpHKpKU",
        "outputId": "eb013304-e6c2-495b-b736-9614bbde9e05"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|emp_name|count|\n",
            "+--------+-----+\n",
            "|  Kaavya|    1|\n",
            "| Radhika|    1|\n",
            "| Varnika|    1|\n",
            "|  Akshay|    1|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What Hadoop gives Spark?\n",
        "\n",
        "\n",
        "1.   YARN resource manager\n",
        "2.   DFS\n",
        "3.   Disaster Recovery capabilities\n",
        "4.   Data Security\n",
        "5.   A distributed data platform\n",
        "6.   Leverage existing clusters\n",
        "7.   Data locality\n",
        "8.   Manage workloads using advanced policies\n",
        "    *   Allocate shares to different teams and users\n",
        "    *   Hierarchical queues\n",
        "    *   Queue placement policies\n",
        "\n",
        "9.   Take advantage of Hadoop's security\n",
        "    *   Run on Kerberized clusters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LK41E9uvN_Zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Frames in Spark\n",
        "\n",
        "\n",
        "*   Unlike an RDD, data is organized into named columns.\n",
        "*   Allowing Spark to manage the schema and only pass data between nodes, in a much more efficient way than using Java serialization\n",
        "*   Data is stored in off-heap memory in binary format\n",
        "*   NO Garbage collection is involved\n",
        "*   Query optimization plan\n",
        "*   Catalyst Query optimizer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wFiqeQT_PXTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark SQL\n",
        "\n",
        "\n",
        "*   Spark SQL is a Spark module for structured data processing\n",
        "\n",
        "*   Connect to any data source the sae way (Hive, Avro, Parquet, ORC, JSON and JDBC etc.)\n",
        "\n",
        "*   Run SQL or HiveQL queries on existing warehouses\n",
        "\n",
        "*   JDBC and ODBC connectivity for business intelligence tools\n",
        "*   There is no reduction in the performance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5SdB9Q5vRH7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example 1: Spark SQL"
      ],
      "metadata": {
        "id": "Mn4tARmnsd_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkyBRVnPseVK",
        "outputId": "844b20e1-7bde-46cd-e78c-25ba4d62a220"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *"
      ],
      "metadata": {
        "id": "eqdGJOCSsqE9"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.config(\"spark.some.config.option\",\"some-value\").getOrCreate()"
      ],
      "metadata": {
        "id": "nGOiKB0FtEJ8"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json('/content/drive/MyDrive/resource/people.json')"
      ],
      "metadata": {
        "id": "hNWzvJLCtQAP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEfUh1Jmtui_",
        "outputId": "685f7379-b800-40b9-d6b9-ead7ff288724"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[age: bigint, name: string]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOIjmxWstv-_",
        "outputId": "21defbaa-da8f-402b-fe7a-d255793c31fc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(age=None, name='Michael'),\n",
              " Row(age=30, name='Andy'),\n",
              " Row(age=19, name='Justin')]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFCbPzMmtzRt",
        "outputId": "eb70dca7-a0e2-4461-bfb3-cea3c931d6e6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|null|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-Ir0RXEt2uI",
        "outputId": "73c75ee8-41dd-4598-a2b9-21fd6e784754"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"name\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OfeITAU83g8",
        "outputId": "c8134767-f31f-4601-ab1f-6db2c4173ee7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|   name|\n",
            "+-------+\n",
            "|Michael|\n",
            "|   Andy|\n",
            "| Justin|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(df['name'], df['age'] + 1).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Nb3DAF0863H",
        "outputId": "7d3efacf-4fe7-4d89-e739-29d4e3c170ae"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|   name|(age + 1)|\n",
            "+-------+---------+\n",
            "|Michael|     null|\n",
            "|   Andy|       31|\n",
            "| Justin|       20|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.select(df['name'], df['age'] + 1)"
      ],
      "metadata": {
        "id": "VbEkr8Op9DDc"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0QWHH9z9Jp6",
        "outputId": "28493daf-0d30-494a-a573-65dbbcd458f8"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[name: string, (age + 1): bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuTR179W9Lui",
        "outputId": "b3e84af3-8ec5-438b-ad4d-0bf403f07d6a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|   name|(age + 1)|\n",
            "+-------+---------+\n",
            "|Michael|     null|\n",
            "|   Andy|       31|\n",
            "| Justin|       20|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df['age'] > 18).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXc3EI9J9NmT",
        "outputId": "e8d2f210-90da-4c48-fc73-7fadd64d1a46"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "|age|  name|\n",
            "+---+------+\n",
            "| 30|  Andy|\n",
            "| 19|Justin|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('age').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAdiGgVQ9S-z",
        "outputId": "39d2b267-a87f-4e1a-c41f-fadd915330f3"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "| age|count|\n",
            "+----+-----+\n",
            "|  19|    1|\n",
            "|null|    1|\n",
            "|  30|    1|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Register the DF as a SQL temp"
      ],
      "metadata": {
        "id": "If_6GTv19bTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"people\")"
      ],
      "metadata": {
        "id": "N9zeeItz9YjQ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sqlDF = spark.sql(\"SELECT * FROM people\")"
      ],
      "metadata": {
        "id": "1SJ8uhoF9jkd"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sqlDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKhAejOG9tYE",
        "outputId": "71f83780-f539-4461-87b7-c63dd317ee2e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[age: bigint, name: string]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sqlDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL9wsRZ_9vUM",
        "outputId": "9faee8de-a942-4537-ad11-5261fed685e2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|null|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM people WHERE age > 18\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhVN2-B191wK",
        "outputId": "5c50365c-830e-4031-c6e1-0652dc8963b1"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "|age|  name|\n",
            "+---+------+\n",
            "| 30|  Andy|\n",
            "| 19|Justin|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "7HlXryzU_h32"
      },
      "execution_count": 60,
      "outputs": []
    }
  ]
}